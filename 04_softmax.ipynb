{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 4: Logistic/Softmax Regression\n",
    "## Associated Reading: Bishop 4.3 (Probabilistic Discriminative Models)\n",
    "\n",
    "# 1 Refactoring the Bayes Classifier\n",
    "For the purposes of classification, we have so far explored a class of model called the Bayes classifier.  This classifier worked by specifying the joint probability distribution of the class and the feature values:\n",
    "$$P(\\mathbf{x},y),$$\n",
    "where $\\mathbf{x}$ is a vector of features, and $y$ is a class label.\n",
    "\n",
    "It was then easy to use Bayes' rule to infer the class:\n",
    "$$\n",
    "P(Y=y|\\mathbf{X}=\\mathbf{x}) \\propto P(\\mathbf{X}=\\mathbf{x}|Y=y)P(Y=y).\n",
    "$$\n",
    "\n",
    "However, this required the specification of a statistical model for each class (what is the distribution of $\\mathbf{X}$ given y).  Consider the lobster example for naive Bayes, in which we assumed that the features $\\mathbf{X}$ were normally distributed (independent from one another), with some mean and covariance that depended on which class they were a member of. \n",
    "$$\n",
    "P(\\mathbf{x}|y) = \\begin{cases} \\mathcal{N}(\\mathbf{x}|\\boldsymbol{\\mu}_0,\\Sigma_0) & \\mathrm{if}\\;y=0, \\\\\n",
    "                                \\mathcal{N}(\\mathbf{x}|\\boldsymbol{\\mu}_1,\\Sigma_1) & \\mathrm{if}\\;y=1 \\end{cases}\n",
    "$$\n",
    "where $\\boldsymbol{\\mu}_1$ and $\\Sigma_1$ are the mean and diagonal covariance matrix of the *survived* class (and conversely for $\\boldsymbol{\\mu}_0$, etc.).  \n",
    "\n",
    "Similarly we had a prior\n",
    "$$\n",
    "P(Y=y) = \\theta^y (1-\\theta)^{1-y}\n",
    "$$\n",
    "which is the probability of a lobster's survival without knowing its size.  We fit our model by finding the maximum likelihood estimators of the various $\\mu$,$\\sigma^2$, and $\\theta$.  \n",
    "\n",
    "This is all well and good, but sometimes it's not so easy to make an assumption about how the data are distributed!  For example, why should lobster size be normally distributed?  How do we even know?  This leads us to an alternative way to perform classification called *discriminative classification*, in which rather than model the features as an explicit statistical model, we'll come up with a function that takes as input the features, and outputs a probability for each class.  \n",
    "\n",
    "What function should we use for such an endeavor?  Let's begin by looking at the two-class naive Bayes model again, where we are interested in the survival probability.  By definition, \n",
    "$$\n",
    "P(Y=1|\\mathbf{x}) = \\frac{P(\\mathbf{x}|Y=1)P(Y=1)}{P(\\mathbf{x}|Y=1)P(Y=1) + P(\\mathbf{x}|Y=0)P(Y=0)}.\n",
    "$$\n",
    "Note that the first term in the denominator is the same as the numerator.  Thus we could simplify this expression by writing:\n",
    "$$\n",
    "P(Y=1|\\mathbf{x}) = \\frac{1}{1 + \\frac{P(\\mathbf{x}|Y=0)P(Y=0)}{P(\\mathbf{x}|Y=1)P(Y=1)}}.\n",
    "$$\n",
    "**What is the remaining term in the denominator called (specifically in the context of horse racing?**\n",
    "<img src=\"images/horse_racing.jpg\" style=\"width: 400px;\">\n",
    "\n",
    "\n",
    "\n",
    "If we make the identification\n",
    "$$\n",
    "a(\\mathbf{x}) = \\ln \\frac{P(\\mathbf{x}|Y=1)P(Y=1)}{P(\\mathbf{x}|Y=0)P(Y=0)},\n",
    "$$\n",
    "we can simplify the expression further to just\n",
    "$$\n",
    "P(Y=1|\\mathbf{x}) = \\frac{1}{1 + \\mathrm{e}^{-a}}.\n",
    "$$\n",
    "The quantity $a$ is called the log-odds.  This is a useful quantity because it squishes the odds down to a symmetric function on the complete real line.  The resulting function, parameterized in terms of $a$ ends up being common enough that it has a name: **the logistic function**:\n",
    "$$\n",
    "\\sigma(a) = \\frac{1}{1 + \\mathrm{e}^{-a}}.\n",
    "$$\n",
    "The logistic function takes a number on the real line, and squashes it down to a value between zero and one, a proper probability.  (Note that you'll also sometimes hear this called a sigmoid, but sigmoid rightly refers to all functions that are vaguely s-shaped).  Here's what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "a = np.linspace(-5,5,101)\n",
    "\n",
    "def sigmoid(a):\n",
    "    return 1./(1+np.exp(-a))\n",
    "\n",
    "plt.plot(a,sigmoid(a))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us some insight into what naive Bayes is doing: we're taking the log-odds of the data for the two classes, and turning it into a class probability by running it through the logistic function.  \n",
    "\n",
    "As it turns out, in this univariate case if we use the naive Bayes model, we can write down exactly what $a$ is as a quadratic function of the features (dropping the bold on $x$, since it's just a scalar):\n",
    "$$\n",
    "a(x) = w_2 x^2 + w_1 x + w_0 ,\n",
    "$$\n",
    "where \n",
    "$$\n",
    "w_2 = \\frac{1}{2}\\left[\\frac{1}{\\sigma^2_0} - \\frac{1}{\\sigma^2_1}\\right]\n",
    "$$\n",
    "$$\n",
    "w_1 = \\frac{\\mu_1}{\\sigma^2_1} - \\frac{\\mu_0}{\\sigma^2_0}\n",
    "$$\n",
    "$$\n",
    "w_0 = -\\frac{1}{2}\\frac{\\mu_1^2}{\\sigma_1^2} + \\frac{1}{2}\\frac{\\mu_0^2}{\\sigma_0^2} + \\ln\\frac{\\pi_1}{\\pi_0}.\n",
    "$$\n",
    "It's important to recognize that by using these definitions, we haven't done anything different from a normal Bayes classifier.  All we've done is rewrite Bayes' theorem in terms of $a$, and $a$ in terms of $w$\n",
    "\n",
    "# 2 Logistic Regression\n",
    "\n",
    "Notice that $\\mathbf{w}$ is written in terms of values of $\\mu$ and $\\sigma$, which themselves are found via maximum likelihood estimation.  Thus, **it was by explicitly modelling the data and the class using the normal and Bernoulli distribution that we are able to write down these values of $\\mathbf{w}$**.  \n",
    "\n",
    "However, what if these values aren't actually the best choice for predicting the data.  There's no big law out there saying that naive Bayes using a normal distribution to model lobster length is the *most accurate classifier*.  Maybe a Gamma distribution would be better (after all, lobster length cannot be negative, yet the normal distribution allows for that).  Or maybe some other exotic thing.  **What if we don't even know what a reasonable distribution would be?**.  \n",
    "\n",
    "When we explicitly set a distribution on the features, we ask the following question: **Given that my features are distributed according to my chosen distribution, what is $a(x)$?**.  An alternative strategy is to ask the question:  **What is the value of $a(x)$ that maximizes the predictive accuracy of the sigmoid?**  Note that this latter question does *not* involve putting a distribution on $\\mathbf{x}$.  Instead, we'll just make a simple assumption about the form of $a(x)$.  \n",
    "\n",
    "A common choice is to assume that $a(x)$ is a linear function of $x$:\n",
    "$$\n",
    "a(x,\\mathbf{w}) = w_0 + w_1 x,\n",
    "$$\n",
    "with parameters $w_0$ and $w_1$ to be determined.  Of course, we can choose whatever we'd like for this model.  For example\n",
    "$$\n",
    "a(x,\\mathbf{w}) = w_0 + w_1 x + \\cdots + w_p x^p\n",
    "$$\n",
    "is an arbitrarily high order polynomial.  \n",
    "\n",
    "Note that this is a very similar model to what we used in linear regression; however recall that this time the output gets sent through the logistic function.\n",
    "$$\n",
    "P(y|x) = \\sigma(x,\\mathbf{w}) = \\frac{1}{1+\\mathrm{e}^{-a(x,\\mathbf{w})}}\n",
    "$$\n",
    "This similarity is where the name *logistic regression* comes from, even though we're not doing regression!  \n",
    "\n",
    "**Now that we have a model specified, how shall we find the weights?**  \n",
    "\n",
    "Just as we did with linear regression, we need to find a function to minimize.  In that content we used sum squared error to measure the difference between our predictions and observations.  However, squared error doesn't really make sense for binary data.  We need an alternative SSE-like thing that is appropriate for a binary classification problem.  How should we get such a thing?  \n",
    "\n",
    "Let's continue with the probabilistic viewpoint, since the output of the sigmoid is a probability.  Recall that for the Bernoulli distribution, we had that \n",
    "$$\n",
    "P(Y=y|\\theta) = \\theta^y (1-\\theta)^{1-y}.\n",
    "$$\n",
    "In circumstances in which $Y=1$ most of the time, $\\theta$ ended up large.  Conversely, when we expected $Y=0$ most of the time (after looking at the data), $\\theta$ was small.  Our situation here isn't too dissimilar: we want $\\theta$ to be close to 1 for cases where the model should predict $Y=1$, and $\\theta$ should be close to zero where the model ought to predict $Y=0$ .  \n",
    "\n",
    "The logistic function gives us a mechanism for doing just that.  It's a function that varies between zero and one and has parameters to tune in order to get different behaviors based on our inputs $x$.  Thus we can write \n",
    "$$\n",
    "P(Y=y|x,\\mathbf{w}) = \\sigma(a)^y (1-\\sigma(a)^{1-y}.\n",
    "$$\n",
    "If we make a bunch of observations of $x$ and $y$, we can cast this as a maximization problem:  Maximize with respect to $\\mathbf{w}$\n",
    "$$\n",
    "P(\\mathbf{Y}=\\mathbf{\\hat{y}}|\\hat{\\mathbf{x}},\\mathbf{w}) = \\prod_{i=1}^m \\sigma(a_i)^{\\hat{y}_i} (1-\\sigma(a_i)^{1-\\hat{y}_i}\n",
    "$$\n",
    "Per usual, it can be tricky to find the gradient of this function that has a product in it.  Instead, we'll find the minimum of the negative log-likelihood:\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{i=1}^m \\hat{y}_i \\ln \\sigma(a_i) + (1-\\hat{y}_i) \\ln (1-\\sigma(a_i)),\n",
    "$$\n",
    "where recall that the hat on $x$ and $y$ indicate that these are *observed* quantities.  \n",
    "\n",
    "The above log-likelihood shows up frequently, and has it's own name: cross-entropy.  It is the natural cost function for binary variables, just as least squares is a natural cost function for real ones: they both come from taking the logarithm of common distributions (**which distribution is least squares related to?**).\n",
    "\n",
    "The derivative of this function is easy to compute, especially given the useful fact that\n",
    "$$\n",
    "\\frac{\\partial \\sigma}{\\partial a} = \\sigma(a) (1-\\sigma(a)).\n",
    "$$\n",
    "Thus we have that\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_j} = \\sum_{i=1}^m \\left[\\frac{\\hat{y}_i}{\\sigma(a_i)} - \\frac{1-\\hat{y}_i}{1-\\sigma(a_i)}\\right]\\frac{\\partial \\sigma(a_i)}{\\partial a_i}\\frac{\\partial a_i}{\\partial w_j}\n",
    "$$\n",
    "which simplifies to \n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_j} = \\sum_{i=1}^m [\\sigma(a_i) - \\hat{y}_i] \\frac{\\partial a_i}{\\partial w_j}.\n",
    "$$\n",
    "\n",
    "Unfortunately, unlike in the case of linear regression, we can't simply set these derivatives to zero and solve a linear system of equations to get the values for $w_i$.  Instead, we'll have to search for the optimal parameter values.  **How should we do this?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
